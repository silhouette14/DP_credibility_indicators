{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpickle .pkl file back to dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"sources/df_articles_ap.pkl\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop rows where len of column['body'] is over API limit 30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['body'].map(len) < 30000]\n",
    "data.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# sum = 0\n",
    "# bigest = 0\n",
    "# counter = 0\n",
    "# for body in data['body']:\n",
    "# #     if(counter < 50):\n",
    "# #         print(body)\n",
    "#     sum += len(body)\n",
    "#     if(len(body) > bigest):\n",
    "#         bigest = len(body)\n",
    "#         print(bigest)\n",
    "#         #print(body)\n",
    "#     counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = data['body'][0]\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analyse\n",
    "\n",
    "tresholdy:\n",
    "\n",
    "- < -4 ; 4 > ---> NEUTRAL\n",
    "- < -5 ; nekonecno ) ---> NEGATIVE\n",
    "- <  5 ; nekonecno ) ---> POSITIVE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt = \"\"\n",
    "api_limit = 29900\n",
    "result_json = \" \"\n",
    "for j in range(len(data)):\n",
    "    try:\n",
    "        if(len(txt) + len(data['body'][j]) >= api_limit or j == len(data) -1 ):\n",
    "\n",
    "            if(j == len(data) -1):\n",
    "                txt = txt + data['body'][j]\n",
    "\n",
    "            url = 'http://arl6.library.sk/nlp4sk/api'\n",
    "            myobj = {'text': txt,\n",
    "                 'apikey': 'DEMOQ',\n",
    "                 'TextPreprocessor': 'BasicTextPreprocessor',\n",
    "                 'postagger': 'DictionaryPOSTagger',\n",
    "                 'tokenizer': 'WhitespaceAndPunctuationTokenizer',\n",
    "                 'sentimentanalyser': 'ExtendedSentimentAnalyser',\n",
    "                 'lemmatizer': 'DictionaryLemmatizer'}\n",
    "\n",
    "            x = requests.post(url, data = myobj)\n",
    "            #print(json.loads(x.text))\n",
    "            time.sleep(3)\n",
    "            \n",
    "            \n",
    "            txt = \"data['body'][j]\"\n",
    "            txt = txt + \"parser \"\n",
    "            \n",
    "\n",
    "            result = json.loads(x.text)\n",
    "            print(type(result))\n",
    "            for i in range(len(result)):\n",
    "                result_json = result_json + result[j]['word']\n",
    "            print(\"Aktualne ID clanku: \", j)\n",
    "\n",
    "        else:\n",
    "            txt = txt + data['body'][j]\n",
    "            txt = txt + \"parser \"\n",
    "            continue\n",
    "    except:     \n",
    "        print(\"Skoncil som na clanku s ID: \", j)\n",
    "        f = open( 'result_json.txt', 'w' )\n",
    "        f.write(result_json)\n",
    "        f.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentiment_column = []\n",
    "txt = \"\"\n",
    "api_limit = 29900\n",
    "\n",
    "for j in range(len(data)):\n",
    "    if(len(txt) + len(data['body'][j]) >= api_limit or j == len(data) -1 ):\n",
    "        \n",
    "        if(j == len(data) -1):\n",
    "            txt = txt + data['body'][j]\n",
    "        \n",
    "        url = 'http://arl6.library.sk/nlp4sk/api'\n",
    "        myobj = {'text': txt,\n",
    "             'apikey': 'DEMOQ',\n",
    "             'TextPreprocessor': 'BasicTextPreprocessor',\n",
    "             'postagger': 'DictionaryPOSTagger',\n",
    "             'tokenizer': 'WhitespaceAndPunctuationTokenizer',\n",
    "             'sentimentanalyser': 'ExtendedSentimentAnalyser',\n",
    "             'lemmatizer': 'DictionaryLemmatizer'}\n",
    "        \n",
    "        x = requests.post(url, data = myobj)\n",
    "        #print(json.loads(x.text))\n",
    "        \n",
    "        \n",
    "        txt = \"data['body'][j]\"\n",
    "        txt = txt + \"parser \"\n",
    "        time.sleep(3)\n",
    "        \n",
    "        result = json.loads(x.text)\n",
    "        #print(result)\n",
    "        \n",
    "        sentiment_score = 0\n",
    "        print(\"cislo clanku: \", j)\n",
    "        for i in range(len(result)):\n",
    "            if(result[i]['sentiment2']):\n",
    "                #print('word: ',result[i]['word'])\n",
    "                #print('sentiment: ',result[i]['sentiment'],\"\\n\")\n",
    "                #print('sentiment2: ',result[i]['sentiment2'],\"\\n\")\n",
    "                if(result[i]['sentiment2'] != \"!\" and result[i]['sentiment2'] != \"/\" and result[i]['sentiment2'] != \"*\"):\n",
    "                    sentiment_score = sentiment_score + int(result[i]['sentiment2'])\n",
    "                    \n",
    "            if(result[i]['word'] == \"parser\"):\n",
    "                #print(\"parser\")\n",
    "                if(sentiment_score >= -4 and sentiment_score <= 4):\n",
    "                    sentiment_column.append(0)\n",
    "                elif(sentiment_score > 4):\n",
    "                    sentiment_column.append(1)\n",
    "                else:\n",
    "                    sentiment_column.append(-1)\n",
    "\n",
    "                sentiment_score = 0\n",
    "                continue\n",
    "    \n",
    "        if(sentiment_score >= -4 and sentiment_score <= 4):\n",
    "            sentiment_column.append(0)\n",
    "        elif(sentiment_score > 4):\n",
    "            sentiment_column.append(1)\n",
    "        else:\n",
    "            sentiment_column.append(-1)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        txt = txt + data['body'][j]\n",
    "        txt = txt + \"parser \"\n",
    "        #print(len(txt))\n",
    "        continue\n",
    "        \n",
    "print(sentiment_column)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data 1000 articles sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 1, -1, 0, 0, 0, 1, -1, 1, -1, 0, 0, -1, 0, 1, 1, 1, 1, 0, 0, 1, 1, -1, 1, -1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, 0, 1, -1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, -1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, -1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 1, 1, -1, 0, 1, -1, 1, 0, 1, 1, 0, 0, -1, 1, 1, -1, 0, 1, 0, 0, 0, 1, 0, 1, -1, 0, 0, 1, 1, 1, 1, -1, 1, 0, 1, 1, -1, 1, 1, 1, 0, -1, 0, 0, -1, 1, 1, -1, -1, -1, 1, 1, 0, 1, 0, -1, 1, 1, 1, 1, -1, 1, 0, 1, -1, 1, 0, 1, -1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, -1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 0, 1, -1, 0, 0, 1, 1, 1, 0, 1, 1, -1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 0, 1, 1, -1, 0, 0, 1, -1, 1, 1, -1, 1, 0, 0, 1, 0, 1, 1, -1, 1, 0, -1, 0, 0, 1, -1, 1, 1, 1, 1, 0, 1, -1, 1, 0, 0, 0, 1, -1, 0, 1, 1, 1, -1, 0, 0, 0, 0, 1, -1, 1, -1, -1, 0, 1, 1, 1, 1, 1, 0, 1, -1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, -1, 0, -1, -1, -1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, -1, 1, 1, 0, -1, 0, -1, 1, 0, 1, 1, 0, -1, 1, -1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, -1, 1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 0, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 0, 1, 1, -1, 0, 1, -1, -1, -1, 1, 0, 0, 1, 0, -1, -1, 1, 1, -1, 1, 1, 1, 1, 0, 1, 0, 1, -1, -1, 1, 1, 0, 1, -1, -1, 1, -1, 1, 0, -1, 0, -1, 0, 1, 0, 1, 1, -1, 0, 1, 1, 1, 1, 1, -1, -1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, -1, 1, 1, 1, 1, 1, 1, 0, -1, -1, -1, -1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, -1, 1, 1, 0, 0, -1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, -1, 1, 1, 0, 1, 1, 1, 0, 0, -1, 1, 0, 1, 1, -1, -1, 0, 1, 0, -1, 1, 1, 1, 1, 0, 1, 1, -1, 0, -1, 1, 0, 0, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 0, -1, 1, 1, 0, 1, 1, 1, -1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, -1, 1, 1, 1, -1, 0, 0, 1, 0, 1, 1, 0, -1, 0, 1, 1, -1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, -1, -1, 1, 1, 1, -1, 0, -1, 0, 1, 1, -1, -1, 0, 0, 0, 0, 0, 1, 1, -1, 0, 1, -1, 1, -1, 1, 0, -1, -1, 1, 1, -1, 1, 0, 0, 1, 1, -1, -1, 1, 1, 0, 1, 1, -1, 1, 0, -1, 1, -1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 0, -1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 0, 1, 1, -1, 1, 1, 1, 0, 1, 1, 1, 0, -1, 1, 1, 1, -1, -1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, -1, 1, -1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, -1, 1, 0, -1, 1, 1, 0, -1, 0, 0, 1, -1, -1, 1, 0, -1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 0, -1, 0, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, -1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, -1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1, -1, -1, 1, 1, 0, 1, 1, 1, -1, 0, -1, 0, 0, 1, 1, -1, 0, 1, 0, 1, -1, 1, 1, -1, 0, 0, -1, 1, -1, -1, 1, -1, 1, 1, 0, 1, 0, 0, -1, 1, 1, 1, 0, 1, 1, 0, 0, 1, -1, -1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, -1, 0, -1, 1, -1, 0, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, -1, -1, 1, 0, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 0, 0, 0, 1, 0, 1, 1, 1, 0, -1, 0, -1, 1, -1, 1, 0, -1, 0, 1, 0, 1, 1, 1, 1, 1, 1, -1, 0, 0, 1, 1, 0, 1, 1, 1, -1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, -1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, -1, 0, 1, -1, 0, 1, 1, 0, -1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_col = []\n",
    "for j in range(0,30):\n",
    "    \n",
    "    txt = data['body'][j]\n",
    "    \n",
    "    url = 'http://arl6.library.sk/nlp4sk/api'\n",
    "    myobj = {'text': txt,\n",
    "             'apikey': 'DEMOQ',\n",
    "             'TextPreprocessor': 'BasicTextPreprocessor',\n",
    "             'postagger': 'DictionaryPOSTagger',\n",
    "             'tokenizer': 'WhitespaceAndPunctuationTokenizer',\n",
    "             'sentimentanalyser': 'ExtendedSentimentAnalyser',\n",
    "             'lemmatizer': 'DictionaryLemmatizer'}\n",
    "\n",
    "    x = requests.post(url, data = myobj)\n",
    "\n",
    "    #print(json.loads(x.text))\n",
    "\n",
    "    result = json.loads(x.text)\n",
    "\n",
    "    #print(result)\n",
    "    sentiment_score = 0\n",
    "    print(\"cislo clanku: \", j)\n",
    "    for i in range(len(result)):\n",
    "        if(result[i]['sentiment2']):\n",
    "            #print('word: ',result[i]['word'])\n",
    "            #print('sentiment: ',result[i]['sentiment'],\"\\n\")\n",
    "            #print('sentiment2: ',result[i]['sentiment2'],\"\\n\")\n",
    "            if(result[i]['sentiment2'] != \"!\" and result[i]['sentiment2'] != \"/\" and result[i]['sentiment2'] != \"*\"):\n",
    "                print(result[i]['sentiment2'])\n",
    "                sentiment_score = sentiment_score + int(result[i]['sentiment2'])\n",
    "\n",
    "    print(sentiment_score)\n",
    "    \n",
    "    if(sentiment_score >= -4 and sentiment_score <= 4):\n",
    "        sentiment_col.append(0)\n",
    "    elif(sentiment_score > 4):\n",
    "        sentiment_col.append(1)\n",
    "    else:\n",
    "        (sentiment_col.append(-1))\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "print(sentiment_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['body'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body'][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
